{"cells":[{"cell_type":"markdown","metadata":{"id":"EfxSuxaBaaOW"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"]},{"cell_type":"markdown","metadata":{"id":"0DNyCgq2VVAU"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/PySpark/9.PySpark_Estimate_Size.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"mmpTAzXGRGLj"},"source":["# Overview\n","\n","Sometimes it is an important question, how much memory does our DataFrame use? And there is no easy answer if you are working with PySpark. You can try to collect the data sample and run local memory profiler. You can estimate the size of the data in the source (for example, in parquet file). But from PySpark API only string representation is available and we will work with it. Please review [this page](https://semyonsinchenko.github.io/ssinchenko/post/estimation-spark-df-size/) for more information.\n"]},{"cell_type":"markdown","metadata":{"id":"HGdS4JP_RGtA"},"source":["### Install PySpark\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3pnMpGRQE2q","outputId":"e05d0c27-1740-4282-af12-811a0a3ec738"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark==3.5.6 in /home/tzhang3/envs/40123/lib/python3.12/site-packages (3.5.6)\n","Requirement already satisfied: py4j==0.10.9.7 in /home/tzhang3/envs/40123/lib/python3.12/site-packages (from pyspark==3.5.6) (0.10.9.7)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# install PySpark\n","%pip install pyspark==3.5.6"]},{"cell_type":"markdown","metadata":{"id":"JTgVVz1xu2B4"},"source":["### Initializing Spark\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"54fC7hk3QeH8","outputId":"d8c9f43b-5a5c-476d-f436-6c6526435c6d","executionInfo":{"status":"ok","timestamp":1758553288108,"user_tz":300,"elapsed":11956,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7a7dbff0f770>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://880b15007e76:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":1}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","spark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p-XaEE_5WmC"},"outputs":[],"source":["#  DO NOT FORGET WHEN YOU'RE DONE => spark.stop()"]},{"cell_type":"markdown","metadata":{"id":"NReZlau33a1U"},"source":["# Pyspark DataFrame\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"OYHuLxXkHb-l","executionInfo":{"status":"ok","timestamp":1758553288362,"user_tz":300,"elapsed":252,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[],"source":["import pandas as pd\n","import io\n","import re\n","import contextlib\n","from pyspark.sql import DataFrame"]},{"cell_type":"markdown","metadata":{"id":"Gyey4DyYrWKS"},"source":["The functions we've defined to calculate estimated sizes.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pQtumFQWYdg2","executionInfo":{"status":"ok","timestamp":1758553288371,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[],"source":["def _bytes2unit(bb: float, unit: str) -> float:\n","    units = {\n","        \"B\": 1,\n","        \"KiB\": 1024,\n","        \"MiB\": 1024 * 1024,\n","        \"GiB\": 1024 * 1024 * 1024,\n","        \"TiB\": 1024 * 1024 * 1024 * 1024,\n","    }\n","    return bb * units[unit]\n","\n","\n","def convert_unit_to_bytes(size: float, unit: str) -> str:\n","    units = {\"B\": \"Byte\", \"KiB\": \"KiB\", \"MiB\": \"MiB\", \"GiB\": \"GiB\", \"TiB\": \"TiB\"}\n","    return f\"{size:.0f} {units[unit]}\"\n","\n","\n","def estimate_size_of_df(df: DataFrame) -> tuple:\n","    \"\"\"Estimate the size of the given DataFrame in different units.\n","    If the size cannot be estimated return (-1.0, -1.0, '').\n","    Sizes are returned in original format, size in bytes, and original unit.\n","\n","    This function works only in PySpark 3.0.0 or higher!\n","\n","    :param df: DataFrame\n","    :returns: Tuple containing original size, size in bytes, and original unit\n","    \"\"\"\n","    with contextlib.redirect_stdout(io.StringIO()) as stdout:\n","        # mode argument was added in 3.0.0\n","        df.explain(mode=\"cost\")\n","\n","    top_line = stdout.getvalue().split(\"\\n\")[1]\n","\n","    # We need a pattern to parse the real size and units\n","    pattern = r\"^.*sizeInBytes=([0-9]+\\.[0-9]+)\\s(B|KiB|MiB|GiB|TiB).*$\"\n","\n","    _match = re.search(pattern, top_line)\n","\n","    if _match:\n","        size = float(_match.groups()[0])\n","        unit = _match.groups()[1]\n","    else:\n","        return -1.0, -1.0, \"\"\n","\n","    return (\n","        size,\n","        _bytes2unit(size, unit),\n","        unit,\n","    )  # original size, size in bytes, original unit"]},{"cell_type":"markdown","metadata":{"id":"-HRfwPQ4rCL7"},"source":["Let's now estimate the sizes of different types of data frames here.\n"]},{"cell_type":"markdown","metadata":{"id":"JXws9c1UKvY-"},"source":["## CSV\n"]},{"cell_type":"code","source":["# Run these lines to fetch the sample dataset if you are on Colab\n","!mkdir -p ./data\n","!wget -q -P ./data https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/PySpark/data/amazonFood.csv"],"metadata":{"id":"jb53GEPs8cXG","executionInfo":{"status":"ok","timestamp":1758553304396,"user_tz":300,"elapsed":1008,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"OPd7wPF1uX1y","executionInfo":{"status":"ok","timestamp":1758553311043,"user_tz":300,"elapsed":6637,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[],"source":["amazon_csv = spark.read.csv(\"./data/amazonFood.csv\", header=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wXLGpQIZTqe","outputId":"55832e88-4e0e-4bb0-f9e7-22d7e794e277","executionInfo":{"status":"ok","timestamp":1758553311059,"user_tz":300,"elapsed":9,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CSV DataFrame size: 22.60 MiB or 23697818 Byte\n"]}],"source":["# Calculate the DataFrame size using the function\n","\n","original_size, size_in_bytes, original_unit = estimate_size_of_df(amazon_csv)\n","\n","if original_size == -1 or size_in_bytes == -1:\n","    print(\"Unable to calculate DataFrame size.\")\n","\n","else:\n","    formatted_original_size = (\n","        \"{:.2f}\".format(original_size)\n","        if original_unit != \"B\"\n","        else \"{:.0f}\".format(original_size)\n","    )\n","    formatted_size_in_bytes = convert_unit_to_bytes(size_in_bytes, original_unit)\n","\n","    if original_unit != \"B\":\n","        print(\n","            f\"CSV DataFrame size: {formatted_original_size} {original_unit} or {size_in_bytes:.0f} Byte\"\n","        )\n","    else:\n","        print(f\"CSV DataFrame size: {formatted_original_size} {original_unit} Byte\")"]},{"cell_type":"markdown","metadata":{"id":"VZBL3TWAwY5H"},"source":["## Parquet\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45wqZRJTsnZX","outputId":"292d1274-af8f-4027-bd4f-e407fe8eb49b","executionInfo":{"status":"ok","timestamp":1758553314624,"user_tz":300,"elapsed":3564,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1.66 ms, sys: 928 Âµs, total: 2.59 ms\n","Wall time: 3.56 s\n"]}],"source":["%%time\n","\n","amazon_csv.write.parquet('./savedData/amazonFood.parquet')\n","\n","amazon_parquet = spark.read.parquet('./savedData/amazonFood.parquet')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwjOxbAMr7mf","outputId":"91e30500-f5f0-4675-9a08-973705a5855b","executionInfo":{"status":"ok","timestamp":1758553314647,"user_tz":300,"elapsed":21,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Parquet DataFrame size: 10.60 MiB or 11114906 Byte\n"]}],"source":["# Calculate the DataFrame size using the function\n","\n","original_size, size_in_bytes, original_unit = estimate_size_of_df(amazon_parquet)\n","\n","if original_size == -1 or size_in_bytes == -1:\n","    print(\"Unable to calculate DataFrame size.\")\n","\n","else:\n","    formatted_original_size = (\n","        \"{:.2f}\".format(original_size)\n","        if original_unit != \"B\"\n","        else \"{:.0f}\".format(original_size)\n","    )\n","    formatted_size_in_bytes = convert_unit_to_bytes(size_in_bytes, original_unit)\n","\n","    if original_unit != \"B\":\n","        print(\n","            f\"Parquet DataFrame size: {formatted_original_size} {original_unit} or {size_in_bytes:.0f} Byte\"\n","        )\n","    else:\n","        print(f\"Parquet DataFrame size: {formatted_original_size} {original_unit} Byte\")"]},{"cell_type":"markdown","metadata":{"id":"mRsAlMB58bW2"},"source":["As seen in the examples above, CSV files occupy approximately twice the space of Parquet files.\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"}},"nbformat":4,"nbformat_minor":0}