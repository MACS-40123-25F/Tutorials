{"cells":[{"cell_type":"markdown","metadata":{"id":"kPuLO3UvExOu"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"]},{"cell_type":"markdown","metadata":{"id":"TNQL5-U1VUY_"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/PySpark/1.PySpark_RDDs.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"YhTUfSoWE2ER"},"source":["# **1. PySpark RDDs**\n"]},{"cell_type":"markdown","metadata":{"id":"mmpTAzXGRGLj"},"source":["# Introduction, Features and Operations of RDD\n","\n","Overview\n","\n","At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n","\n","A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only “added” to, such as counters and sums.\n","\n","This guide shows each of these features in each of Spark’s supported languages. It is easiest to follow along with if you launch Spark’s interactive shell – either bin/spark-shell for the Scala shell or bin/pyspark for the Python one.\n","[source](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n"]},{"cell_type":"markdown","metadata":{"id":"HGdS4JP_RGtA"},"source":["### Install PySpark\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3pnMpGRQE2q","outputId":"3f52f80c-457b-46d2-bdaf-c116b113df14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark==3.5.6 in /home/tzhang3/envs/40123/lib/python3.12/site-packages (3.5.6)\n","Requirement already satisfied: py4j==0.10.9.7 in /home/tzhang3/envs/40123/lib/python3.12/site-packages (from pyspark==3.5.6) (0.10.9.7)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# install PySpark\n","%pip install pyspark==3.5.6"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"bcaqM7QpR6VO","executionInfo":{"status":"ok","timestamp":1758549211628,"user_tz":300,"elapsed":451,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[],"source":["import pyspark"]},{"cell_type":"markdown","metadata":{"id":"JTgVVz1xu2B4"},"source":["### Initializing Spark\n"]},{"cell_type":"markdown","metadata":{"id":"uC61FDQgR6VP"},"source":["In PySpark, the entry point to programming Spark with the Dataset and DataFrame API is the `SparkSession` class.\n","\n","You might also see examples that use `SparkContext`, which was the entry point to Spark functionality in Spark 1.x.\n","\n","To compare, here's a table that highlights the differences between `SparkSession` and `SparkContext`:\n","\n","| Feature           | SparkSession                       | SparkContext                |\n","| ----------------- | ---------------------------------- | --------------------------- |\n","| Introduced in     | Spark 2.0                          | Spark 1.x                   |\n","| API Level         | High-level (DataFrames, SQL)       | Low-level (RDDs)            |\n","| Supports SQL      | Yes                                | No                          |\n","| Underlying engine | Wraps and uses SparkContext        | Core engine                 |\n","| Used for          | DataFrame, Dataset, SQL, Streaming | RDDs and cluster operations |\n","\n","#### TL;DR\n","\n","In conclusion, if you're working with DataFrames, Datasets, or SQL, you should use `SparkSession`.\n","\n","If you're working with only RDDs (e.g., `sc.parallelize` and `rdd.map`, `rdd.filter`, etc.), you might still use `SparkContext`, but it's generally recommended to use `SparkSession` for new applications.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"elapsed":10597,"status":"ok","timestamp":1758549224697,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"54fC7hk3QeH8","outputId":"5666ae2a-84c2-4366-b475-c7c286290959"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<SparkContext master=local[*] appName=PySparkTutorial>"],"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://78be8ef2b041:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkTutorial</code></dd>\n","            </dl>\n","        </div>\n","        "]},"metadata":{},"execution_count":7}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"PySparkTutorial\").getOrCreate()\n","sc = spark.sparkContext\n","sc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p-XaEE_5WmC"},"outputs":[],"source":["# ==>> DO NOT FORGET WHEN YOU'RE DONE>> spark.stop()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1758549224717,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"zm1PV6B0Q0Ub","outputId":"2e7a5ccc-742b-4d65-8339-5fd2d446c7a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["PySpark version:  3.5.1\n","Python version:   3.12\n","local[*]\n"]}],"source":["print(\"PySpark version: \", spark.version)\n","# 3.2.0\n","\n","# Python Version: To retrieve Python version of SparkContext\n","print(\"Python version:  \", spark.sparkContext.pythonVer)\n","\n","# Master: URL of the cluster or “local” string to run in local mode of SparkContext\n","print(spark.sparkContext.master)"]},{"cell_type":"markdown","metadata":{"id":"UwQ8aRWsR6VQ"},"source":["### **[Exercise]** Compare SparkSession and SparkContext\n","\n","Run the code below to create a `SparkContext` instance and print its version, Python version, and master URL. Observe that `SparkSession` now encapsulates `SparkContext`, so you can access it via `spark.sparkContext` if you have a `SparkSession` instance.\n","\n","#### Code\n","\n","```python\n","from pyspark import SparkContext\n","\n","sc = SparkContext(appName=\"PySparkTutorial\", master= \"local[*]\")\n","sc = SparkContext.getOrCreate()\n","sc\n","\n","# PySpark version: To retrieve PySpark version of SparkContext\n","print(\"PySpark version: \", sc.version)\n","\n","# Python Version: To retrieve Python version of SparkContext\n","print(\"Python version:  \", sc.pythonVer)\n","\n","# Master: URL of the cluster or “local” string to run in local mode of SparkContext\n","print(sc.master)\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"Y7jJTGnMUB5p"},"source":["# Spark RDD (Resillient Distributed Datasets)\n","\n","Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n"]},{"cell_type":"markdown","metadata":{"id":"0p38J7Pf3QGP"},"source":["### Creating RDDs\n","\n","There are two ways to create RDDs,\n","\n","- parallelizing an existing collection of objects in your driver program,\n","\n","- External datasets (referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.)\n"]},{"cell_type":"markdown","metadata":{"id":"jsp4zOKvvrwp"},"source":["#### Parallelized Collections\n","\n","Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1518,"status":"ok","timestamp":1758549252803,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"DK9JHl9vTNtQ","outputId":"5725fc5f-3c80-4d5b-cd27-c27cb1906335"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pyspark.rdd.RDD'>\n"]},{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3, 4, 5]"]},"metadata":{},"execution_count":9}],"source":["# parallelize() for creating RDDs from python lists\n","\n","data = [1, 2, 3, 4, 5]\n","distData = sc.parallelize(data)\n","print(type(distData))\n","\n","# collect() to retrieve all elements of the RDD\n","# Do not use collect() on large RDDs as it may cause out of memory error on the driver\n","distData.collect()"]},{"cell_type":"markdown","metadata":{"id":"MpGMpwmZv2QV"},"source":["#### External Datasets\n","\n","Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n","\n","Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:\n"]},{"cell_type":"code","source":["# Run these lines to fetch the sample dataset if you are on Colab\n","!mkdir -p ./data\n","!wget -q -P ./data https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/PySpark/data/airport-codes.csv\n","!wget -q -P ./data https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/PySpark/data/news_category_test.csv"],"metadata":{"id":"RQTQ8BlnsiuK","executionInfo":{"status":"ok","timestamp":1758549265443,"user_tz":300,"elapsed":1127,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"wOCTDI4mYeb4","executionInfo":{"status":"ok","timestamp":1758549267459,"user_tz":300,"elapsed":339,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[],"source":["# textFile() for creating RDDs from existing file\n","\n","rdd = sc.textFile(\"./data/news_category_test.csv\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1206,"status":"ok","timestamp":1758549269964,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"dIi_s9ruTNoJ","outputId":"58fee0df-6698-4bdd-9c5f-f9f51b8865e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['category,description',\n"," \"Business,Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n"," 'Sci/Tech,\" TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for privately funded suborbital space flight, has officially announced the first launch date for its manned rocket.\"',\n"," 'Sci/Tech,\" A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\"',\n"," 'Sci/Tech,\" It\\'s barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\"',\n"," 'Sci/Tech,\" Southern California\\'s smog fighting agency went after emissions of the bovine variety Friday, adopting the nation\\'s first rules to reduce air pollution from dairy cow manure.\"',\n"," 'Sci/Tech,\"The British Department for Education and Skills (DfES) recently launched a \"\"Music Manifesto\"\" campaign, with the ostensible intention of educating the next generation of British musicians. Unfortunately, they also teamed up with the music industry (EMI, and various artists) to make this popular. EMI has apparently negotiated their end well, so that children in our schools will now be indoctrinated about the illegality of downloading music.The ignorance and audacity of this got to me a little, so I wrote an open letter to the DfES about it. Unfortunately, it\\'s pedantic, as I suppose you have to be when writing to goverment representatives. But I hope you find it useful, and perhaps feel inspired to do something similar, if or when the same thing has happened in your area.\"',\n"," 'Sci/Tech,\"confessed author of the Netsky and Sasser viruses, is responsible for 70 percent of virus infections in 2004, according to a six month virus roundup published Wednesday by antivirus company Sophos.\"\"  \"\"The 18 year old Jaschan was taken into custody in Germany in May by police who said he had admitted programming both the Netsky and Sasser worms, something experts at Microsoft confirmed. (A Microsoft antivirus reward program led to the teenager\\'s arrest.) During the five months preceding Jaschan\\'s capture, there were at least 25 variants of Netsky and one of the port scanning network worm Sasser.\"\"  \"\"Graham Cluley, senior technology consultant at Sophos, said it was staggeri ...  \"',\n"," 'Sci/Tech,\\\\\\\\FOAF/LOAF  and bloom filters have a lot of interesting properties for social\\\\network and whitelist distribution.\\\\\\\\I think we can go one level higher though and include GPG/OpenPGP key\\\\fingerpring distribution in the FOAF file for simple web-of-trust based key\\\\distribution.\\\\\\\\What if we used FOAF and included the PGP key fingerprint(s) for identities?\\\\This could mean a lot.  You include the PGP key fingerprints within the FOAF\\\\file of your direct friends and then include a bloom filter of the PGP key\\\\fingerprints of your entire whitelist (the source FOAF file would of course need\\\\to be encrypted ).\\\\\\\\Your whitelist would be populated from the social network as your client\\\\discovered new identit ...\\\\\\\\',\n"," 'Sci/Tech,\"Wiltshire Police warns about \"\"phishing\"\" after its fraud squad chief was targeted.\"']"]},"metadata":{},"execution_count":12}],"source":["rdd.collect()[:10]"]},{"cell_type":"markdown","metadata":{"id":"67fvtfgW1r8L"},"source":["## RDDs Operations\n"]},{"cell_type":"markdown","metadata":{"id":"p9nF9Vl0ze5S"},"source":["RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n","\n","By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n","\n","#### Common RDD Transformations\n","\n","**All transformations in Spark are lazy**, The transformations are only computed when an action requires a result to be returned to the driver program.\n","\n","| **Transformations (lazy)** | **Documentation**                                                                                                        |\n","| -------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n","| `map`                      | [RDD.map() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.map.html)                 |\n","| `filter`                   | [RDD.filter() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.filter.html)           |\n","| `flatMap`                  | [RDD.flatMap() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.flatMap.html)         |\n","| `reduceByKey`              | [RDD.reduceByKey() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.reduceByKey.html) |\n","| `join`                     | [RDD.join() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.join.html)               |\n","| `cogroup`                  | [RDD.cogroup() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.cogroup.html)         |\n","| `repartition`              | [RDD.repartition() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.repartition.html) |\n","\n","#### Common RDD Actions\n","\n","**Actions are eager operations that immediately trigger the execution of the transformations required** to compute the RDD. Actions return values to the driver program or write data to an external storage system.\n","\n","| **Actions (eager)** | **Documentation**                                                                                                                  |\n","| ------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n","| `count`             | [RDD.count() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.count.html)                       |\n","| `reduce`            | [RDD.reduce() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.reduce.html)                     |\n","| `collect`           | [RDD.collect() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.collect.html)                   |\n","| `take`              | [RDD.take() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.take.html)                         |\n","| `saveAsTextFile`    | [RDD.saveAsTextFile() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.saveAsTextFile.html)     |\n","| `saveAsHadoopFile`  | [RDD.saveAsHadoopFile() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.saveAsHadoopFile.html) |\n","| `countByValue`      | [RDD.countByValue() – PySpark](https://spark.apache.org/docs/3.5.6/api/python/reference/api/pyspark.RDD.countByValue.html)         |\n"]},{"cell_type":"markdown","metadata":{"id":"dciFK6keR6VR"},"source":["### Examples\n"]},{"cell_type":"markdown","metadata":{"id":"ysJGLOLi9F8a"},"source":["- collect() : Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1758549276272,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"p_oGVjQyAeJV","outputId":"8f1b47b7-e12d-4b6c-cdf2-7378107ec5c3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['category,description',\n"," \"Business,Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n"," 'Sci/Tech,\" TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for privately funded suborbital space flight, has officially announced the first launch date for its manned rocket.\"',\n"," 'Sci/Tech,\" A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\"',\n"," 'Sci/Tech,\" It\\'s barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\"']"]},"metadata":{},"execution_count":13}],"source":["rdd.collect()[:5]"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1758549277183,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"WArodBdlOlx5","outputId":"72915bc8-790d-439a-a3af-2f149a57e9f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 36.4 ms, sys: 3 ms, total: 39.4 ms\n","Wall time: 197 ms\n"]}],"source":["%%time\n","%%capture\n","rdd.collect()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1697,"status":"ok","timestamp":1758549279597,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"nI8m2Nu1gbng","outputId":"41b76d0a-756b-489b-a233-f09b7582f0da"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","8\n","27\n","64\n"]}],"source":["numbRDD = sc.parallelize([0, 1, 2, 3, 4])\n","\n","# Create map() transformation to cube numbers\n","cubedRDD = numbRDD.map(lambda x: x**3)\n","\n","# Collect the results\n","numbers_all = cubedRDD.collect()\n","\n","# Print the numbers from numbers_all\n","for numb in numbers_all:\n","    print(numb)"]},{"cell_type":"markdown","metadata":{"id":"dytKUdIE17aE"},"source":["- filter() : Return a new dataset formed by selecting those elements of the source on which func returns true.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1758549280816,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"kcsr2EIrTNlo","outputId":"c7f0c952-0067-47d2-a8f6-0e96c70697b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Business,Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n"," 'Business,\" Apparel retailers are hoping their back to school fashions will make the grade among style conscious teens and young adults this fall, but it could be a tough sell, with students and parents keeping a tighter hold on their wallets.\"',\n"," 'Business,\" The dollar dipped to a four week low  against the euro on Monday before rising slightly on  profit taking, but steep oil prices and weak U.S. data  continued to fan worries about the health of the world\\'s  largest economy.\"',\n"," 'Business,\" U.S. Treasury debt prices slipped on  Monday, though traders characterized the move as profit taking  rather than any fundamental change in sentiment.\"',\n"," 'Business, The dollar extended gains against the  euro on Monday after a report on flows into U.S. assets showed  enough of a rise in foreign investments to offset the current  account gap for the month.',\n"," \"Business, The dollar held steady near this week's  four week low against the euro on Tuesday with investors  awaiting a German investor confidence survey and U.S. consumer  inflation numbers to shed light on the direction.\",\n"," 'Business,\"  In the latest of a series of product delays, Intel Corp. has postponed the launch of a video display chip it had previously planned to introduce by year end, putting off a showdown with Texas Instruments Inc. in the fast growing market for high definition television displays.\"',\n"," \"Business, Shares of Kmart Holding Corp. surged 17 percent Monday after the discount retailer reported a profit for the second quarter and said chairman and majority owner Edward Lampert is now free to invest the company's  #36;2.6 billion in surplus cash.\",\n"," 'Business,Keep an eye on your credit card issuers -- they may be about to raise your rates.',\n"," 'Business,\" Money managers are growing more pessimistic about the economy, corporate profits and US stock market returns, according to a monthly survey by Merrill Lynch released Tuesday. \"']"]},"metadata":{},"execution_count":16}],"source":["rdd.filter(lambda x: \"Business\" in x).collect()[:10]"]},{"cell_type":"markdown","metadata":{"id":"KkfNJrRR1N8N"},"source":["- count() Return the number of elements in the dataset.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":492,"status":"ok","timestamp":1758549282058,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"2o-PuMzV9R56","outputId":"e862da8b-7433-4104-e267-c08565116fc8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7601"]},"metadata":{},"execution_count":17}],"source":["rdd.count()"]},{"cell_type":"markdown","metadata":{"id":"nNt234CP1odY"},"source":["- map() Return a new distributed dataset formed by passing each element of the source through a function func\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1266,"status":"ok","timestamp":1758549283498,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"Gn_zHIaC9hJB","outputId":"84b9e1a6-1b3e-4b92-b48f-b5921456db8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["7601\n","[20, 136, 234, 221, 279]\n"]}],"source":["LineLength = rdd.map(lambda x: len(x))\n","print(LineLength.count())\n","print(LineLength.collect()[:5])"]},{"cell_type":"markdown","metadata":{"id":"zIbPpdfS2Aym"},"source":["- take(n) Return an array with the first n elements of the dataset.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579,"status":"ok","timestamp":1758549284078,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"iROYtx3SAf4q","outputId":"c801e03e-0a94-490d-a7e3-5a93d1c54b97"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['category,description',\n"," \"Business,Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n"," 'Sci/Tech,\" TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for privately funded suborbital space flight, has officially announced the first launch date for its manned rocket.\"',\n"," 'Sci/Tech,\" A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\"',\n"," 'Sci/Tech,\" It\\'s barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\"']"]},"metadata":{},"execution_count":19}],"source":["rdd.take(5)"]},{"cell_type":"markdown","metadata":{"id":"kkGhlIwybqUX"},"source":["## Partition, Repartition and Coalesce\n"]},{"cell_type":"markdown","metadata":{"id":"q0cnHbHI2TQz"},"source":["- saveAsTextFile(path) Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"UT321RfMEqnd","executionInfo":{"status":"ok","timestamp":1758549284778,"user_tz":300,"elapsed":16,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[],"source":["# delete existing savedData folder\n","#! rm -R savedData"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2601,"status":"ok","timestamp":1758549287738,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"DuKp4SifCQLa","outputId":"ea7e6a5f-c202-46eb-aade-df8b76397cad"},"outputs":[{"output_type":"stream","name":"stdout","text":["5\n"]}],"source":["# Understanding Partitioning in PySpark\n","# Save RDD Data to HDFS\n","# textFile() method\n","\n","rdd3 = sc.textFile(\"./data/airport-codes.csv\", minPartitions=5)\n","\n","rdd3.saveAsTextFile(\"./savedData/\")\n","#  hadoop fs -cat /savedData/part-00000\n","#  hadoop fs -cat /savedData/part-00001\n","#  hadoop fs -cat /savedData/part-00002\n","#  hadoop fs -cat /savedData/part-00003\n","#  hadoop fs -cat /savedData/part-00004\n","\n","print(rdd3.getNumPartitions())"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1758549287755,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"fM4VOZTZ8Eft","outputId":"d0780b43-1db5-4891-fd1b-efe20de9efcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}],"source":["# parallelize() method\n","\n","numRDD = sc.parallelize(range(10), numSlices=3)\n","\n","print(numRDD.getNumPartitions())"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":508,"status":"ok","timestamp":1758549288264,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"C6oe8ud9wAFM","outputId":"1a46224b-c27d-43f5-91ea-a0065c0e1f91"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"]}],"source":["print(numRDD.collect())"]},{"cell_type":"markdown","metadata":{"id":"u_ymQ30_g_Rx"},"source":["- glom() return an RDD created by coalescing all elements within each partition into a list. https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1758549289359,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"EQDelLlav_8x","outputId":"4024237a-560f-4155-f785-369bcf707b3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of partitions: 2\n","Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"]}],"source":["# Default Partition Number is 2\n","\n","rdd = sc.parallelize(range(10))\n","\n","print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n","print(\"Partitions structure: {}\".format(rdd.glom().collect()))"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":634,"status":"ok","timestamp":1758549289994,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"17vtnsSEeDaS","outputId":"b707c43e-3cd6-45c9-cadd-6237eb296cc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of partitions: 4\n","Partitions structure: [[0, 1], [2, 3, 4], [5, 6], [7, 8, 9]]\n"]}],"source":["rdd = sc.parallelize(range(10), numSlices=4)\n","\n","print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n","print(\"Partitions structure: {}\".format(rdd.glom().collect()))"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":597,"status":"ok","timestamp":1758549290612,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"Lj9JfXnZv__t","outputId":"b72d1d53-c77d-4653-ddf1-5ef70e343558"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of partitions: 4\n","Partitions structure: [[0, 1], [2, 3, 4], [5, 6], [7, 8, 9]]\n"]}],"source":["rdd = sc.parallelize(range(10), 4)\n","\n","print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n","print(\"Partitions structure: {}\".format(rdd.glom().collect()))"]},{"cell_type":"markdown","metadata":{"id":"augeFhWzhuaD"},"source":["- if you are increasing the number of partitions use **repartition()** (performing full shuffle),\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2238,"status":"ok","timestamp":1758549292868,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"OjLXSb8ZcQnN","outputId":"7e86295a-e4d0-4e7a-9b7c-f24ef4b83a5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of partitions: 5\n","Partitions structure: [[], [], [], [7, 8, 9], [0, 1, 2, 3, 4, 5, 6]]\n"]}],"source":["repartRdd = rdd.repartition(5)\n","\n","print(\"Number of partitions: {}\".format(repartRdd.getNumPartitions()))\n","print(\"Partitions structure: {}\".format(repartRdd.glom().collect()))"]},{"cell_type":"markdown","metadata":{"id":"mn2xzrS8iWfd"},"source":["- If you are decreasing the number of partitions in this RDD, consider\n","  using **coalesce()**, which can avoid performing a shuffle.\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":809,"status":"ok","timestamp":1758549293687,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"PYqHrzUzhnP7","outputId":"540b6b6b-4e0a-4c84-baf7-9166ea572a3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of partitions: 2\n","Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"]}],"source":["repartRdd = rdd.coalesce(2)\n","\n","print(\"Number of partitions: {}\".format(repartRdd.getNumPartitions()))\n","print(\"Partitions structure: {}\".format(repartRdd.glom().collect()))"]},{"cell_type":"markdown","metadata":{"id":"QT9OfmM_3v7S"},"source":["## Passing Functions to Spark\n","\n","Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are three recommended ways to do this:\n","\n","Lambda expressions, for simple functions that can be written as an expression. (Lambdas do not support multi-statement functions or statements that do not return a value.)\n","Local defs inside the function calling into Spark, for longer code.\n","Top-level functions in a module.\n","For example, to pass a longer function than can be supported using a lambda, consider the code below:\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73,"status":"ok","timestamp":1758549293768,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"F88gbsWL0XWM","outputId":"da0335da-a121-4989-91bf-fe84eb5618e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonRDD[32] at RDD at PythonRDD.scala:53"]},"metadata":{},"execution_count":29}],"source":["\"\"\"MyScript.py\"\"\"\n","\n","if __name__ == \"__main__\":\n","\n","    def myFunc(s):\n","\n","        return len(s)\n","\n","\n","rdd3.map(myFunc)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1655,"status":"ok","timestamp":1758549295422,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"MR0cYHfUv_yc","outputId":"191f7a76-4100-4b90-ed4e-e6706b44a91f"},"outputs":[{"output_type":"stream","name":"stdout","text":["55114\n","[116, 100, 98, 101, 106]\n"]}],"source":["print(rdd3.map(myFunc).count())\n","\n","print(rdd3.map(myFunc).collect()[:5])"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1758549295441,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"8DMx4mGk9CWK","outputId":"299e3b2b-ef2e-4a7f-c7bd-8ba40ae11618"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['p', 's', 'r', 'p']"]},"metadata":{},"execution_count":31}],"source":["# Read a CSV File\n","# Writing a Python Function to Parse CSV Lines\n","import csv\n","from io import StringIO\n","\n","\n","def parseCSV(csvRow):\n","    data = StringIO(csvRow)\n","    dataReader = csv.reader(data, lineterminator=\"\")\n","    return next(dataReader)\n","\n","\n","csvRow = \"p,s,r,p\"\n","parseCSV(csvRow)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1758549295671,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"TA8V5GVRv_vx","outputId":"18bf3021-4bf2-41a5-a5b6-e3f42d05ddbe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['ident',\n","  'type',\n","  'name',\n","  'elevation_ft',\n","  'continent',\n","  'iso_country',\n","  'iso_region',\n","  'municipality',\n","  'gps_code',\n","  'iata_code',\n","  'local_code',\n","  'coordinates']]"]},"metadata":{},"execution_count":32}],"source":["# Read csv file and Creating a Paired RDD\n","filamentRDD = sc.textFile(\"./data/airport-codes.csv\", 4)\n","filamentRDDCSV = filamentRDD.map(parseCSV)\n","filamentRDDCSV.take(1)"]},{"cell_type":"markdown","metadata":{"id":"iminBGIF-53f"},"source":["## Anonymous functions\n","\n","Lambda functions are anonymous functions in Python\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1758549295993,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"hbe54aa7-5qT","outputId":"9302b7b4-ad6e-4cc2-fbe2-30a7ae6471e1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0,\n"," 0.8414709848078965,\n"," 0.9995736030415051,\n"," 0.9092974268256817,\n"," 0.1411200080598672,\n"," 0.0015926529164868282]"]},"metadata":{},"execution_count":33}],"source":["## Transformations (lazy evaluation)\n","\n","# map() transformation applies a function to all elements in the RDD\n","\n","import math\n","\n","numRDD = sc.parallelize(range(100), numSlices=3)\n","\n","RDD = sc.parallelize([0, 1, 1.6, 2, 3, 3.14])\n","RDD_map = RDD.map(lambda x: math.sin(x))\n","\n","RDD_map.collect()"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":387,"status":"ok","timestamp":1758549296381,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"WfejfBIqv_tc","outputId":"c049ec3b-549c-4896-a7a7-7c7d88bbe20d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 3, 4]"]},"metadata":{},"execution_count":34}],"source":["# filter() transformation returns a new RDD with only the elements that pass the condition\n","\n","RDD = sc.parallelize([0, 1, 2, 3, 4])\n","\n","RDD_filter = RDD.filter(lambda x: x >= 2)\n","\n","RDD_filter.collect()"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1758549296741,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"LI9E5xIZv_of","outputId":"3dd89960-e28a-41ae-e853-7aecdd174482"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hello', 'world', 'how', 'are', 'you']"]},"metadata":{},"execution_count":35}],"source":["# flatMap() transformation returns multiple values for each element in the original RDD\n","\n","\"\"\"\n","Why are we using flatMap, rather than map?\n","\n","The reason is that the operation line.split(\" \") generates a list of strings,\n","so had we used map the result would be an RDD of lists of words. Not an RDD of words.\n","\n","The difference between map and flatMap is that the second expects to get a list as the result\n","from the map and it concatenates the lists to form the RDD.\n","\"\"\"\n","\n","RDD = sc.parallelize([\"hello world\", \"how are you\"])\n","\n","RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\n","\n","RDD_flatmap.collect()"]},{"cell_type":"markdown","metadata":{"id":"lxLFGPuXvPKs"},"source":["## Introduction to pair RDDs in PySpark\n","\n","Two common ways to create pair RDDs\n","\n","    From a list of key-value tuple\n","    From a regular RDD\n","\n","Get the data into key/value form for paired RDD\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1758549297727,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"gKH0cL581oZQ","outputId":"d7e38021-1074-403d-a6bb-a328d1c9d971"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Sam', 23), ('Mary', 34), ('Peter', 25)]"]},"metadata":{},"execution_count":36}],"source":["my_tuple = [(\"Sam\", 23), (\"Mary\", 34), (\"Peter\", 25)]\n","\n","pairRDD_tuple = sc.parallelize(my_tuple)\n","\n","pairRDD_tuple.collect()"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1758549298660,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"5nswbWawvR6c","outputId":"d3e4ff21-01b2-458d-db92-5cc624b71fa9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Sam', '23'), ('Mary', '34'), ('Peter', '25')]"]},"metadata":{},"execution_count":37}],"source":["my_list = [\"Sam 23\", \"Mary 34\", \"Peter 25\"]\n","\n","regularRDD = sc.parallelize(my_list)\n","\n","pairRDD_RDD = regularRDD.map(lambda s: (s.split(\" \")[0], s.split(\" \")[1]))\n","\n","pairRDD_RDD.collect()"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1758549299195,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"9QIq48MWvR4V","outputId":"6823d292-99e4-4761-83d8-604e3e3b06e7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['23', '34', '25']"]},"metadata":{},"execution_count":38}],"source":["#  Fetching Values from a Paired RDD\n","pairRDD_RDD_Values = pairRDD_RDD.values()\n","pairRDD_RDD_Values.collect()"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1758549299753,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"teZQCmGTvR1o","outputId":"67fd15a4-8121-49c7-f562-db6721708cee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Sam', 'Mary', 'Peter']"]},"metadata":{},"execution_count":39}],"source":["#  Fetching Keys from a Paired RDD\n","pairRDD_RDD_Keys = pairRDD_RDD.keys()\n","pairRDD_RDD_Keys.collect()"]},{"cell_type":"markdown","metadata":{"id":"qgYQAwZniNPk"},"source":["## Join\n"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":377,"status":"ok","timestamp":1758549300727,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"baSCTs6ziQER","outputId":"2c96aa7d-0c9b-4f18-8203-0077aca05470"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['name', 'age', 'occupation'],\n"," ['Alex', '25', 'student'],\n"," ['Mary', '30', 'scientist'],\n"," ['Brown', '45', 'teacher']]"]},"metadata":{},"execution_count":40}],"source":["people_rdd = sc.parallelize(\n","    [\n","        \"name, age, occupation\",\n","        \"Alex, 25, student\",\n","        \"Mary, 30, scientist\",\n","        \"Brown, 45, teacher\",\n","    ]\n",")\n","\n","transactions_rdd = sc.parallelize(\n","    [\n","        \"product, price, name\",\n","        \"Book, 10, Brown\",\n","        \"Schoolbag, 12, Alex\",\n","        \"Pen, 4, Brown\",\n","        \"Mouse, 8, Mary\",\n","    ]\n",")\n","\n","\n","# Expand each row from one long string into a list of discrete entries\n","people_rdd = people_rdd.map(lambda line: line.split(\", \"))\n","transactions_rdd = transactions_rdd.map(lambda line: line.split(\", \"))\n","\n","people_rdd.collect()"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1356,"status":"ok","timestamp":1758549302093,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"BZrID8YKnM_F","outputId":"a90640ce-5aac-4e0a-efba-d426c862b416"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Book', '10', 'Brown'],\n"," ['Schoolbag', '12', 'Alex'],\n"," ['Pen', '4', 'Brown'],\n"," ['Mouse', '8', 'Mary']]"]},"metadata":{},"execution_count":41}],"source":["# Remove header row, since RDD operations are low-level Spark operations that do not rely on schema\n","people_header = people_rdd.first()\n","transactions_header = transactions_rdd.first()\n","\n","people_rdd = people_rdd.filter(lambda line: line != people_header)\n","transactions_rdd = transactions_rdd.filter(lambda line: line != transactions_header)\n","\n","people_rdd.collect()\n","transactions_rdd.collect()"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1758549302357,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"_PEK6BfJnXzt","outputId":"79863bf8-e6e3-49e7-fd65-4f47346573d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Alex', '25, student'), ('Mary', '30, scientist'), ('Brown', '45, teacher')]"]},"metadata":{},"execution_count":42}],"source":["# Format each RDD as (K, V) to prepare for the join operation\n","\n","people_rdd = people_rdd.map(lambda line: (line[0], line[1] + \", \" + line[2]))\n","transactions_rdd = transactions_rdd.map(\n","    lambda line: (line[2], line[0] + \", \" + line[1])\n",")\n","\n","people_rdd.collect()"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1768,"status":"ok","timestamp":1758549304124,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"fKOAxaxvnbJh","outputId":"08f2d3be-448d-493a-c525-0c26f7995c7c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Mary', ('30, scientist', 'Mouse, 8')),\n"," ('Alex', ('25, student', 'Schoolbag, 12')),\n"," ('Brown', ('45, teacher', 'Book, 10')),\n"," ('Brown', ('45, teacher', 'Pen, 4'))]"]},"metadata":{},"execution_count":43}],"source":["join = people_rdd.join(transactions_rdd)\n","join.collect()"]},{"cell_type":"markdown","metadata":{"id":"RqNk897gv0Hc"},"source":["## Transformations on pair RDDs\n","\n","All regular transformations work on pair RDD\n","\n","Have to pass functions that operate on tuples rather than on individual elements\n","\n","Examples of paired RDD Transformations\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1758549304430,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"gHJnV6ByvRwa","outputId":"89fa8688-46d1-4634-96d1-973164c16853"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 4, 9, 16, 9, 4, 1, 0]"]},"metadata":{},"execution_count":44}],"source":["# we can use user functions to map on RDD\n","\n","\n","def get_Squares(num):\n","    return num**2\n","\n","\n","numbRDD = sc.parallelize([0, 1, 2, 3, 4, 3, 2, 1, 0])\n","\n","numbRDD.map(get_Squares).collect()"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":568,"status":"ok","timestamp":1758549305013,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"RRzWd55RvRtz","outputId":"ef67676d-90f3-49a3-b822-b8a10da70fda"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 2, 4, 1, 3]"]},"metadata":{},"execution_count":45}],"source":["# distinct() to find the distinct numbers\n","\n","numbRDD.distinct().collect()"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1689,"status":"ok","timestamp":1758549306701,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"2SDWJDqPvRrE","outputId":"75a6ce22-ac35-4cc1-961d-8998064468f2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3]"]},"metadata":{},"execution_count":46}],"source":["#  intersection()\n","\n","numbRDD2 = sc.parallelize([1, 2, 3, 5])\n","\n","numbRDD.intersection(numbRDD2).collect()"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3037,"status":"ok","timestamp":1758549309752,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"3ZaSiQg2vRow","outputId":"50f3adea-baab-4f19-b07e-fbcedcef3f5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","99\n","4950\n","49.5\n","833.25\n","28.86607004772212\n","(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)\n","{'count': 100, 'mean': 49.5, 'sum': 4950.0, 'min': np.float64(0.0), 'max': np.float64(99.0), 'stdev': np.float64(29.011491975882016), 'variance': 841.6666666666666}\n"]}],"source":["# calculating basic stats\n","\n","numbRDD = sc.parallelize([1, 2, 3, 4, 2, 5, 1])\n","\n","print(numRDD.min())\n","\n","print(numRDD.max())\n","\n","print(numRDD.sum())\n","\n","print(numRDD.mean())\n","\n","print(numRDD.variance())\n","\n","print(numRDD.stdev())\n","\n","print(numRDD.stats())\n","\n","print(numRDD.stats().asDict())"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":572,"status":"ok","timestamp":1758549310336,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"89__LU-RvRlr","outputId":"95b258c7-2b7d-4d77-e6e6-f6dfd3dfb3c8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Messi', 47), ('Ronaldo', 58), ('Neymar', 46)]"]},"metadata":{},"execution_count":48}],"source":["# reduceByKey() transformation combines values with the same key\n","\n","# It runs parallel operations for each key in the dataset\n","\n","# It is a transformation and not action\n","\n","regularRDD = sc.parallelize(\n","    [\n","        (\"Messi\", 23),\n","        (\"Ronaldo\", 34),\n","        (\"Neymar\", 22),\n","        (\"Messi\", 24),\n","        (\"Ronaldo\", 24),\n","        (\"Neymar\", 24),\n","    ]\n",")\n","\n","pairRDD_reducebykey = regularRDD.reduceByKey(lambda x, y: x + y)\n","\n","pairRDD_reducebykey.collect()"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":548,"status":"ok","timestamp":1758549310883,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"ryaSc2UyvRkI","outputId":"c7a069ce-9ef5-4d84-9675-0025f7530785"},"outputs":[{"output_type":"stream","name":"stdout","text":["UK ['LHR']\n","US ['JFK', 'SFO']\n","FR ['CDG']\n"]}],"source":["# groupbykey() groups all the values with the same key in the pair RDD\n","\n","airports = [(\"US\", \"JFK\"), (\"UK\", \"LHR\"), (\"FR\", \"CDG\"), (\"US\", \"SFO\")]\n","\n","regularRDD = sc.parallelize(airports)\n","\n","pairRDD_group = regularRDD.groupByKey().collect()\n","\n","for cont, air in pairRDD_group:\n","    print(cont, list(air))"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1822,"status":"ok","timestamp":1758549312704,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"6ndZv5LavRgc","outputId":"055b3755-5b1f-40d2-99da-344f74c33626"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('a', (1, 2)), ('a', (1, 3)), ('c', (2, 3)), ('d', (6, 5))]"]},"metadata":{},"execution_count":50}],"source":["# join() transformation joins the two pair RDDs based on their key\n","\n","x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"c\", 2), (\"d\", 6)])\n","\n","y = sc.parallelize([(\"a\", 2), (\"a\", 3), (\"c\", 3), (\"d\", 5)])\n","\n","sorted(x.join(y).collect())"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":506,"status":"ok","timestamp":1758549313209,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"lm731dtUvRd0","outputId":"cb3282be-a359-4ef6-b575-17d4283e33bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["15\n"]}],"source":["# reduce(func) action is used for aggregating the elements of a regular RDD\n","\n","# The function should be commutative and associative\n","\n","# An example of reduce() action in PySpark\n","\n","from operator import add\n","\n","print(sc.parallelize([1, 2, 3, 4, 5]).reduce(add))"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1032,"status":"ok","timestamp":1758549314240,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"PemA8RekvRbR","outputId":"8589b908-333c-4a3e-f70a-fca9372ab727"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":52}],"source":["sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":382,"status":"ok","timestamp":1758549314700,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"Iaw3BytP3aU7","outputId":"7cfefce3-05da-455d-d75d-828db2c35380"},"outputs":[{"output_type":"stream","name":"stdout","text":["a 3\n","b 2\n"]}],"source":["# countByKey() only available for type (K, V)\n","\n","# countByKey() action counts the number of elements for each key\n","\n","# Example of countByKey() on a simple list\n","\n","rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2), (\"b\", 3), (\"a\", 1)])\n","\n","for key, val in rdd.countByKey().items():\n","    print(key, val)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123,"status":"ok","timestamp":1758549314846,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"SKtlSrVz3aSL","outputId":"f8f7a76a-20af-4d65-c097-f4a0964c0255"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{1: 2, 3: 4}"]},"metadata":{},"execution_count":54}],"source":["# collectAsMap() return the key-value pairs in the RDD as a dictionary\n","\n","# Example of collectAsMap() on a simple tuple\n","\n","sc.parallelize([(1, 2), (3, 4)]).collectAsMap()"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4662,"status":"ok","timestamp":1758549320297,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"Wdv3oaU-3aPm","outputId":"4bdb84c2-6bbc-4c36-df1f-de1eeebe6d0c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1871, 'Hospital'),\n"," (1392, 'Air'),\n"," (1314, 'Municipal'),\n"," (1247, 'Ranch'),\n"," (1023, 'de'),\n"," (1009, 'Center'),\n"," (1003, 'Seaplane'),\n"," (1000, 'International'),\n"," (872, 'County'),\n"," (761, 'Medical'),\n"," (672, 'Regional'),\n"," (609, 'Lake'),\n"," (582, 'Farm'),\n"," (529, 'Memorial'),\n"," (497, 'Landing'),\n"," (451, 'De'),\n"," (385, 'do'),\n"," (373, 'Creek'),\n"," (360, 'Island'),\n"," (334, 'Do')]"]},"metadata":{},"execution_count":55}],"source":["# word count example\n","\n","text_file = sc.textFile(\"./data/airport-codes.csv\")\n","counts_rdd = (\n","    text_file.flatMap(lambda line: line.split(\" \"))\n","    .map(lambda word: (word, 1))\n","    .reduceByKey(lambda a, b: a + b)\n",")\n","\n","# print the word frequencies in descending order\n","\n","counts_rdd.map(lambda x: (x[1], x[0])).sortByKey(ascending=False).collect()[:20]"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4705,"status":"ok","timestamp":1758549325003,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"N3p223S73aNa","outputId":"a4753d3b-ef9f-4dd4-b973-5f9fedeee0b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(683, ('Medical', 'Center')),\n"," (282, ('Memorial', 'Hospital')),\n"," (257, ('di', 'Volo')),\n"," (192, ('Lake', 'Seaplane')),\n"," (130, ('Regional', 'Medical')),\n"," (124, ('Community', 'Hospital')),\n"," (110, ('Air', 'Force')),\n"," (101, ('General', 'Hospital')),\n"," (83, ('Building', 'Heliport,,AS,KR,KR-11,Seoul,,,,\"37')),\n"," (68, ('County', 'Hospital'))]"]},"metadata":{},"execution_count":56}],"source":["# bigrams and word frequencies\n","\n","sentences = (\n","    sc.textFile(\"./data/airport-codes.csv\")\n","    .glom()\n","    .map(lambda x: \" \".join(x))\n","    .flatMap(lambda x: x.split(\".\"))\n",")\n","\n","bigrams = sentences.map(lambda x: x.split()).flatMap(\n","    lambda x: [((x[i], x[i + 1]), 1) for i in range(0, len(x) - 1)]\n",")\n","\n","freq_bigrams = (\n","    bigrams.reduceByKey(lambda x, y: x + y).map(lambda x: (x[1], x[0])).sortByKey(False)\n",")\n","\n","freq_bigrams.take(10)"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"g6vXWtpMDPUd","executionInfo":{"status":"ok","timestamp":1758549325012,"user_tz":300,"elapsed":8,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}}},"outputs":[],"source":["# spark.stop()"]},{"cell_type":"markdown","metadata":{"id":"PRTtLKJYwAeF"},"source":["# Resources\n","\n","1. https://spark.apache.org/docs/latest/rdd-programming-guide.html\n","2. https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#\n","3. https://github.com/vkocaman/PySpark_Essentials_March_2019\n","4. https://github.com/sundarramamurthy/pyspark\n","5. https://towardsdatascience.com/beginners-guide-to-pyspark-bbe3b553b79f\n","6. https://www.guru99.com/pyspark-tutorial.html\n","7. https://towardsdatascience.com/exploratory-data-analysis-eda-with-pyspark-on-databricks-e8d6529626b1\n","8. https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"}},"nbformat":4,"nbformat_minor":0}