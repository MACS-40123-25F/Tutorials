{"cells":[{"cell_type":"markdown","metadata":{"id":"byKB1hSXABlj"},"source":["# Finding Similar Wikipedia Articles with MinHashLSH\n","\n","This notebook demonstrates how to use the MinHashLSH (Locality Sensitive Hashing) algorithm in PySpark to find similar documents in a text-based dataset.\n","\n","**Credits**\n","\n","Adapted from [Detecting Abuse at Scale: Locality Sensitive Hashing at Uber Engineering](https://www.uber.com/blog/lsh/)\n","\n","**Compatibility**\n","\n","| Platform                     | Compatible | Recommended | Notes                                                                                                                                                         |\n","| ---------------------------- | ---------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| **Local (e.g., M1 MacBook)** | ✅ Yes      | ✅ Yes       | -  |\n","| **Google Colab**             | ✅ Yes      | ✅ Yes       |  - |\n","| **Midway3 Login Node**       | ✅ Yes      | ✅ Yes       | - |\n","| **Midway3 Compute Node**     | ✅ Yes      | ✅ Yes      | -  |\n"]},{"cell_type":"markdown","metadata":{"id":"tFz23VRWABlk"},"source":["## 1. Load Raw Data\n","\n","First, we need a Spark session. In a real-world scenario on a computing cluster like Amazon EMR, you would load data from a distributed file system like HDFS. For this local demonstration, we'll create a sample DataFrame directly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocJKHqfkABlk"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import BooleanType\n","from pyspark.ml.feature import Tokenizer, CountVectorizer, MinHashLSH\n","from pyspark.ml.linalg import Vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQF8J5WCABll","executionInfo":{"status":"ok","timestamp":1758554303064,"user_tz":300,"elapsed":28344,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"f30699cd-d658-44a5-8cb8-7b3bed0fb4b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Wikipedia articles:\n","+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|title         |content                                                                                                                                                    |\n","+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|United States |The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.|\n","|USA           |The U.S. is a federal republic and a representative democracy with three separate branches of government.                                                  |\n","|Canada        |Canada is a country in North America. Its ten provinces and three territories extend from the Atlantic to the Pacific and northward into the Arctic Ocean. |\n","|Germany       |Germany is a country in Central Europe. It is the second-most populous country in Europe after Russia.                                                     |\n","|United Kingdom|The United Kingdom, made up of England, Scotland, Wales and Northern Ireland, is an island nation in northwestern Europe.                                  |\n","|America       |The term America (or the Americas) refers to the landmasses of North and South America. The United States is often referred to as America.                 |\n","+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"MinHashLSH_Tutorial\").getOrCreate()\n","\n","# For this notebook, we'll create a sample DataFrame to simulate the raw data\n","sample_data = [\n","    (\n","        \"United States\",\n","        \"The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\",\n","    ),\n","    (\n","        \"USA\",\n","        \"The U.S. is a federal republic and a representative democracy with three separate branches of government.\",\n","    ),\n","    (\n","        \"Canada\",\n","        \"Canada is a country in North America. Its ten provinces and three territories extend from the Atlantic to the Pacific and northward into the Arctic Ocean.\",\n","    ),\n","    (\n","        \"Germany\",\n","        \"Germany is a country in Central Europe. It is the second-most populous country in Europe after Russia.\",\n","    ),\n","    (\n","        \"United Kingdom\",\n","        \"The United Kingdom, made up of England, Scotland, Wales and Northern Ireland, is an island nation in northwestern Europe.\",\n","    ),\n","    (\n","        \"America\",\n","        \"The term America (or the Americas) refers to the landmasses of North and South America. The United States is often referred to as America.\",\n","    ),\n","]\n","df_sample = spark.createDataFrame(sample_data, [\"title\", \"content\"])\n","\n","print(\"Sample Wikipedia articles:\")\n","df_sample.show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"BPzJwcw5ABlm"},"source":["---\n"]},{"cell_type":"markdown","metadata":{"id":"7E4pr6bkABlm"},"source":["## 2. Prepare Feature Vectors\n","\n","**MinHashLSH** works with feature vectors. We need to convert the text content of each article into a numerical vector. This process involves two main steps:\n","\n","1.  **Tokenization**: Splitting the article content into individual words.\n","2.  **Vectorization**: Converting the list of words into a feature vector of word counts using `CountVectorizer`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIpVl-JPABlm","executionInfo":{"status":"ok","timestamp":1758554308896,"user_tz":300,"elapsed":5830,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"c7a8222f-16ac-4fb5-e040-017870aeabf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Articles converted to feature vectors:\n","+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|title         |features                                                                                                                                                          |\n","+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|United States |(76,[0,1,2,3,5,6,7,8,9,10,12,13,15,30,31,34,37,48,60,65,68,73],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |\n","|USA           |(76,[0,1,2,4,6,16,18,26,27,40,41,54,55,59,69],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |\n","|Canada        |(76,[0,1,2,3,4,5,7,11,12,16,20,24,32,38,39,44,46,50,58,63,70,71,72],[3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n","|Germany       |(76,[0,1,2,3,5,14,22,29,35,42,47,51,52,75],[1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                             |\n","|United Kingdom|(76,[0,1,3,4,6,8,14,21,23,25,36,43,49,56,57,61,64,67,74],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |\n","|America       |(76,[0,1,4,6,7,8,9,11,12,13,15,17,19,28,33,45,53,62,66],[4.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                            |\n","+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["# Tokenize the article content\n","tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n","words_df = tokenizer.transform(df_sample)\n","\n","# Vectorize the words for each article\n","vocab_size = 1000\n","cv = CountVectorizer(\n","    inputCol=\"words\", outputCol=\"features\", vocabSize=vocab_size, minDF=1.0\n",")\n","cv_model = cv.fit(words_df)\n","\n","# Filter out any articles that might have resulted in empty vectors\n","vectorized_df = (\n","    cv_model.transform(words_df)\n","    .filter(col(\"features\").isNotNull())\n","    .select(col(\"title\"), col(\"features\"))\n",")\n","print(\"Articles converted to feature vectors:\")\n","vectorized_df.show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"AbCyVmr6ABln"},"source":["---\n"]},{"cell_type":"markdown","metadata":{"id":"HDP_xVFVABlo"},"source":["## 3. Fit and Query the LSH Model\n","\n","Now we can fit our MinHashLSH model to the vectorized data. The `numHashTables` parameter allows us to balance between performance and accuracy.\n","\n","PySpark's [`MinHashLSH`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinHashLSH.html) is a powerful tool for finding approximate nearest neighbors in large datasets, especially for comparing sets of items, like the words in documents.\n","- It takes a DataFrame as **input**, where one column (specified by `setInputCol`) contains feature vectors, which are typically sparse vectors representing sets (e.g., word counts from a document).\n","- After fitting the `MinHashLSH` model, it produces a new DataFrame with an **output** column (specified by `setOutputCol`) containing the hash values for each input vector.\n","\n","The main argument, `numHashTables`, is the equivalent of `bands` discussed in the **illustrated guide**: more hash tables increase the accuracy of finding similar items but also increase computation time."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_kEvnuNABlo","executionInfo":{"status":"ok","timestamp":1758554310601,"user_tz":300,"elapsed":1702,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"ff718eea-84c6-4a6a-eb92-6ec84ba18802"},"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed data with hash values:\n","+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------+\n","|title         |features                                                                                                                                                          |hashValues                                     |\n","+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------+\n","|United States |(76,[0,1,2,3,5,6,7,8,9,10,12,13,15,30,31,34,37,48,60,65,68,73],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |[[4.4752084E7], [8.6080663E7], [3.074331E7]]   |\n","|USA           |(76,[0,1,2,4,6,16,18,26,27,40,41,54,55,59,69],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |[[9.9367721E7], [2.7990005E7], [3.3515818E7]]  |\n","|Canada        |(76,[0,1,2,3,4,5,7,11,12,16,20,24,32,38,39,44,46,50,58,63,70,71,72],[3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[[1.53983358E8], [2.7990005E7], [1.127336E8]]  |\n","|Germany       |(76,[0,1,2,3,5,14,22,29,35,42,47,51,52,75],[1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                             |[[2.7144192E7], [1.44171321E8], [1.50956237E8]]|\n","|United Kingdom|(76,[0,1,3,4,6,8,14,21,23,25,36,43,49,56,57,61,64,67,74],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |[[4.4752084E7], [5.7035334E7], [3.074331E7]]   |\n","|America       |(76,[0,1,4,6,7,8,9,11,12,13,15,17,19,28,33,45,53,62,66],[4.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                            |[[9536300.0], [2.28624684E8], [3.074331E7]]    |\n","+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------+\n","\n"]}],"source":["# Initialize and fit the MinHashLSH model\n","minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashValues\", numHashTables=3)\n","lsh_model = minhash.fit(vectorized_df)\n","\n","# Feature Transformation: Show the hash values for each article\n","print(\"Transformed data with hash values:\")\n","hashed_df = lsh_model.transform(vectorized_df)\n","\n","hashed_df.show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"odV0mEjOABlp"},"source":["### Query the Model\n","\n","Once you have a fitted [`MinHashLSHModel`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinHashLSHModel.html), you can use it to find similar items in two primary ways:\n","\n","* **`approxNearestNeighbors`**: This function is like a \"search\" operation. You provide a single feature vector (the \"key\") that you want to find matches for, and an integer **k**, and it will efficiently search through the entire dataset to return the top **k** items that are most similar to your key. The similarity is measured by the Jaccard distance between the items' feature vectors.\n","\n","* **`approxSimilarityJoin`**: This function is used to find all pairs of similar items *within* a dataset (or between two different datasets) that meet a certain similarity level. You provide a **threshold** (a value between 0 and 1 for the Jaccard distance), and it returns all pairs of items from the datasets whose distance is less than or equal to that threshold. It's incredibly efficient for tasks like large-scale document deduplication or finding all related product pairs in a massive catalog.\n","\n","> ⚠: The `approxSimilarityJoin` uses Jaccard distance (`1 - similarity`) instead of similarity as threshold, so you need to set the threshold accordingly."]},{"cell_type":"markdown","metadata":{"id":"lKkRRAuqABlp"},"source":["### Approximate Nearest Neighbor Search\n","\n","We can now use the trained LSH model to find the most similar articles to a given key. For this example, we'll create a key vector representing the phrase \"united states\" and search for the top 3 most similar articles."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OL9t8VIJABlp","executionInfo":{"status":"ok","timestamp":1758554311880,"user_tz":300,"elapsed":1272,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"59d31d40-0160-40fe-c2ad-06379c0010b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Approximate nearest neighbors for 'united states':\n","+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------+------------------+\n","|title         |features                                                                                                                                                 |hashValues                                  |distCol           |\n","+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------+------------------+\n","|America       |(76,[0,1,4,6,7,8,9,11,12,13,15,17,19,28,33,45,53,62,66],[4.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |[[9536300.0], [2.28624684E8], [3.074331E7]] |0.8947368421052632|\n","|United States |(76,[0,1,2,3,5,6,7,8,9,10,12,13,15,30,31,34,37,48,60,65,68,73],[2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[[4.4752084E7], [8.6080663E7], [3.074331E7]]|0.9090909090909091|\n","|United Kingdom|(76,[0,1,3,4,6,8,14,21,23,25,36,43,49,56,57,61,64,67,74],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                  |[[4.4752084E7], [5.7035334E7], [3.074331E7]]|0.95              |\n","+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------+------------------+\n","\n"]}],"source":["# Create a key vector for the query \"united states\"\n","key_vector_indices = [\n","    cv_model.vocabulary.index(word)\n","    for word in [\"united\", \"states\"]\n","    if word in cv_model.vocabulary\n","]\n","search_key = Vectors.sparse(\n","    vocab_size, sorted(list(set([(i, 1.0) for i in key_vector_indices])))\n",")\n","\n","# Find the 3 nearest neighbors\n","num_neighbors = 3\n","print(f\"Approximate nearest neighbors for 'united states':\")\n","lsh_model.approxNearestNeighbors(hashed_df, search_key, num_neighbors).show(\n","    truncate=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"jqSa6kxpABlp"},"source":["### Approximate Similarity Join\n","\n","To find all pairs of articles in the dataset that are similar to each other, we can use an approximate similarity join. We set a `threshold` for the Jaccard distance to determine which pairs are considered \"similar\".\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvJmAWsBABlp","executionInfo":{"status":"ok","timestamp":1758554316245,"user_tz":300,"elapsed":4360,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"59adf939-542f-4acc-aaa8-a7a58290dd07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pairs of articles with a Jaccard distance <= 0.8:\n","+-------------+-------------+---------------+\n","|Title A      |Title B      |JaccardDistance|\n","+-------------+-------------+---------------+\n","|America      |United States|0.71875        |\n","|United States|America      |0.71875        |\n","+-------------+-------------+---------------+\n","\n"]}],"source":["# Perform a self-join to find similar pairs of articles within the dataset\n","distance_threshold = 0.8  # Jaccard distance threshold\n","print(f\"Pairs of articles with a Jaccard distance <= {distance_threshold}:\")\n","lsh_model.approxSimilarityJoin(\n","    hashed_df, hashed_df, distance_threshold, distCol=\"JaccardDistance\"\n",").filter(\"JaccardDistance > 0\").select(\n","    col(\"datasetA.title\").alias(\"Title A\"),\n","    col(\"datasetB.title\").alias(\"Title B\"),\n","    col(\"JaccardDistance\"),\n",").show(\n","    truncate=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"zruKjnqIABlp"},"source":["---\n"]},{"cell_type":"markdown","metadata":{"id":"IqUN4fmJABlp"},"source":["## 4. Performance and efficiency\n","\n","As noted in the original tutorial, LSH provides a significant performance benefit over brute-force methods. The trade-off between speed and accuracy, tunable via the `numHashTables` parameter, makes LSH a powerful tool for large-scale similarity detection tasks. 🚀\n","\n","- **Approximate Nearest Neighbor Search**: Can be significantly faster (e.g., 2x) than a full scan.\n","- **Approximate Similarity Join**: Can be 3x-5x faster than a full cross-join and filter.\n","\n","This speedup is achieved while maintaining high accuracy, making LSH a practical and efficient solution for finding similar items in massive datasets.\n"]},{"cell_type":"markdown","metadata":{"id":"VsF6FOBZABlp"},"source":["<img src=\"./assets/lsh_perf.avif\" alt=\"Performance Comparison of LSH vs. Brute Force\" width=\"800\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nSZHONgiABlp"},"outputs":[],"source":["# Stop the Spark Session\n","spark.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9JbzEcaABlp"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}